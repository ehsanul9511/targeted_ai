import numpy as np
import pandas as pd
from imblearn.metrics import geometric_mean_score
from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score, roc_curve, f1_score, matthews_corrcoef, confusion_matrix
from sklearn.metrics import auc
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import model_utils
from data_utils import normalize
from whitebox_attack import Top10CorrNeurons, wb_corr_attacks, make_neuron_output_data
import torch

def get_indices_by_conditions(ds, df, conditions):
    new_df = ds.ds.original_df.copy()
    new_df = new_df.iloc[df.index].copy()
    for col, cond in conditions.items():
        new_df = new_df[new_df[col] == cond]
    indices = new_df.index
    return indices

def get_accuracy(ds, conditions, clf, X, y, metric = 'accuracy'):
    indices = get_indices_by_conditions(ds, X, conditions)
    X = X.loc[indices,:]
    y = y[indices]
    if metric == 'accuracy':
        return np.argmax(clf.predict_proba(X), axis=1)
        acc = 100 * clf.score(X, ds.ds.sensitive_enc.transform(y.to_numpy().ravel().reshape(-1,1)).toarray())
    elif metric == 'precision':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        acc = precision_score(y, y_pred) * 100
    elif metric == 'recall':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        acc = recall_score(y, y_pred) * 100
    elif metric == 'auc':
        y_pred = clf.predict_proba(X)
        y_pred = y_pred[:,1]
        acc = roc_auc_score(y, y_pred) * 100
    elif metric == 'f1':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        acc = f1_score(y, y_pred) * 100
    elif metric == 'mcc':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        acc = matthews_corrcoef(y, y_pred) * 100
    elif metric == 'gmean':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        acc = geometric_mean_score(y, y_pred) * 100
    elif metric == 'fpr':
        y_pred = clf.predict(X)
        y_pred = np.argmax(y_pred, axis=1)
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        fpr = fp / (fp + tn)
        acc = fpr * 100
    return acc


def subgroup_vulnerability_distance_vector(ds, conditions, X):
    cond = conditions.copy()
    data_dict = ds.ds.meta
    cond[data_dict['sensitive_column']] = data_dict['sensitive_positive']
    indices = get_indices_by_conditions(ds, X, cond).to_numpy()
    X_neuron_positive = X.loc[indices,:].to_numpy()
    X_neuron_positive_mean = np.mean(X_neuron_positive, axis=0)
    cond = conditions.copy()
    sensitive_negative = [val for val in data_dict['sensitive_values'] if val != data_dict['sensitive_positive']][0]
    cond[data_dict['sensitive_column']] = sensitive_negative
    indices = get_indices_by_conditions(ds, X, cond).to_numpy()
    X_neuron_negative = X.loc[indices,:].to_numpy()
    X_neuron_negative_mean = np.mean(X_neuron_negative, axis=0)
    
    # calculate the distance between the two groups
    return X_neuron_positive_mean - X_neuron_negative_mean


def subgroup_vulnerability_distance(ds, conditions, X_neuron):
    distance_vector = subgroup_vulnerability_distance_vector(ds, conditions, X_neuron)
    distance = np.linalg.norm(distance_vector)
    return distance.squeeze()


def get_subgroup_disparity(ds, subgroup_columns, neuron_clf, X_neuron, y, metric='accuracy'):

    subgroup_disparity_dict = {}

    for col in subgroup_columns:
        values = ds.ds.unique_values_dict[col]
        subgroup_asrs = {}
        subgroup_dists = {}

        for val in values:
            if len(get_indices_by_conditions(ds, X_neuron, {col:val})) > 500:
                subgroup_asrs[val] = get_accuracy(ds, {col:val}, neuron_clf, X_neuron, y, metric=metric)
                subgroup_dists[val] = subgroup_vulnerability_distance(ds, {col:val}, X_neuron)

        subgroup_disparity_dict[col] = {'asrs':subgroup_asrs, 'dists':subgroup_dists}

    return subgroup_disparity_dict


def get_subgroup_disparity_baseline(ds, subgroup_columns, X_neuron, y, metric='accuracy'):

    subgroup_disparity_dict = {}

    for col in subgroup_columns:
        values = ds.ds.unique_values_dict[col]
        subgroup_asrs = {}
        subgroup_dists = {}

        for val in values:
            if len(get_indices_by_conditions(ds, X_neuron, {col:val})) > 500:
                X_neuron_subgroup = X_neuron.loc[get_indices_by_conditions(ds, X_neuron, {col:val}),:]
                y_subgroup = y[get_indices_by_conditions(ds, X_neuron, {col:val}).to_numpy()]

                x_n_tr, x_n_te, y_n_tr, y_n_te = train_test_split(X_neuron_subgroup, y_subgroup, test_size=0.2, random_state=42)
                top_10_corr_neurons_model = wb_corr_attacks(x_n_tr, y_n_tr)
                top_10_neuron_indices = top_10_corr_neurons_model.corrs_top_10
                # wb_preds = top_10_corr_neurons_model(torch.from_numpy(X_neuron).float()).detach().numpy()
                wb_preds = top_10_corr_neurons_model(torch.from_numpy(x_n_te.to_numpy()).float()).detach().numpy()
                # draw the ROC curve
                y_wb_att = y_n_te.ravel().astype(np.float32)
                subgroup_asrs[val] = roc_auc_score(y_wb_att, wb_preds) * 100
                subgroup_dists[val] = subgroup_vulnerability_distance(ds, {col:val}, X_neuron.loc[:,top_10_neuron_indices])

        subgroup_disparity_dict[col] = {'asrs':subgroup_asrs, 'dists':subgroup_dists}

    return subgroup_disparity_dict

def get_imputation_prediction(**kwargs):
    X_att_query = kwargs.get('X_att_query', None)
    y_att_query = kwargs.get('y_att_query', None)
    subgroup_columns = kwargs.get('subgroup_columns', None)
    # metric = kwargs.get('metric', 'accuracy')
    subgroup_disparity_dict = {}
    clf = kwargs.get('clf', None)

    return np.argmax(clf.predict_proba(X_att_query), axis=1)
    # get_accuracy(ds, {}, clf, X_att_query, y_att_query, metric=metric)


def get_disparity_by_subgroup(attack_type='LOMIA', **kwargs):
    ds = kwargs.get('ds', None)
    X_att_query = kwargs.get('X_att_query', None)
    y_att_query = kwargs.get('y_att_query', None)
    subgroup_columns = kwargs.get('subgroup_columns', None)
    metric = kwargs.get('metric', 'accuracy')
    subgroup_disparity_dict = {}


    if attack_type in ['NEUR_OURS', 'NEUR_IMP']:
        df = kwargs.get('df', None)
        MLP = kwargs.get('MLP', None)
        X_att_query = make_neuron_output_data(ds, df, MLP, ds.ds.y_columns)

    if attack_type in ['LOMIA', 'NEUR_OURS', 'INV', 'PCA']:
        clf = kwargs.get('clf', None)

            # X_att_query = make_neuron_output_data(ds, X_att_query, y_att_query, ds.ds.y_columns)

        for col in subgroup_columns:
            values = ds.ds.unique_values_dict[col]
            subgroup_asrs = {}

            for val in values:
                if len(get_indices_by_conditions(ds, X_att_query, {col:val})) > 500:
                    subgroup_asrs[val] = get_accuracy(ds, {col:val}, clf, X_att_query, y_att_query, metric=metric)

            subgroup_asrs['original'] = get_accuracy(ds, {}, clf, X_att_query, y_att_query, metric=metric)

            subgroup_disparity_dict[col] = subgroup_asrs

    elif attack_type in ['NEUR_IMP']:
        clf = kwargs.get('clf', None)

        for col in subgroup_columns:
            values = ds.ds.unique_values_dict[col]
            subgroup_asrs = {}

            subgroup_tuples = [(val, {col:val}) for val in values]
            subgroup_tuples.append(('original', {}))

            for val, condition in subgroup_tuples:
                if len(get_indices_by_conditions(ds, X_att_query, condition)) > 500:
                    X_att_query_subgroup = X_att_query.loc[get_indices_by_conditions(ds, X_att_query, condition),:]
                    y_att_query_subgroup = y_att_query[get_indices_by_conditions(ds, X_att_query, condition).to_numpy()]

                    top_10_corr_neurons_model = clf
                    top_10_neuron_indices = top_10_corr_neurons_model.corrs_top_10
                    y_preds = top_10_corr_neurons_model(torch.from_numpy(X_att_query_subgroup.to_numpy()).float()).detach().numpy()

                    # check if y_preds crosses the threshold
                    if metric in ['accuracy', 'precision', 'recall', 'fpr', 'f1']:
                        y_preds = np.where(y_preds > top_10_corr_neurons_model.threshold, 1, 0)

                    if metric == 'accuracy':
                        subgroup_asrs[val] = accuracy_score(y_att_query_subgroup, y_preds) * 100
                    elif metric == 'precision':
                        subgroup_asrs[val] = precision_score(y_att_query_subgroup, y_preds) * 100
                    elif metric == 'recall':
                        subgroup_asrs[val] = recall_score(y_att_query_subgroup, y_preds) * 100
                    elif metric == 'f1':
                        subgroup_asrs[val] = f1_score(y_att_query_subgroup, y_preds) * 100
                    elif metric == 'auc':
                        subgroup_asrs[val] = roc_auc_score(y_att_query_subgroup, y_preds) * 100
                    elif metric == 'fpr':
                        tn, fp, fn, tp = confusion_matrix(y_att_query_subgroup, y_preds).ravel()
                        fpr = fp / (fp + tn)
                        subgroup_asrs[val] = fpr * 100

            

            subgroup_disparity_dict[col] = subgroup_asrs

    elif attack_type in ['GRAD', 'CONF']:
        y_pred = kwargs.get('y_pred', None)
        threshold = kwargs.get('threshold', None)

        for col in subgroup_columns:
            values = ds.ds.unique_values_dict[col]
            subgroup_asrs = {}

            subgroup_tuples = [(val, {col:val}) for val in values]
            subgroup_tuples.append(('original', {}))

            for val, condition in subgroup_tuples:
                if len(get_indices_by_conditions(ds, X_att_query, condition)) > 500:
                    X_att_query_subgroup = X_att_query.loc[get_indices_by_conditions(ds, X_att_query, condition),:]
                    y_att_query_subgroup = y_att_query[get_indices_by_conditions(ds, X_att_query, condition).to_numpy()]
                    y_pred_subgroup = y_pred[get_indices_by_conditions(ds, X_att_query, condition).to_numpy()]

                    if metric in ['accuracy', 'precision', 'recall']:
                        y_pred_subgroup = np.where(y_pred_subgroup > threshold, 1, 0)

                    if metric == 'accuracy':
                        subgroup_asrs[val] = accuracy_score(y_att_query_subgroup, y_pred_subgroup) * 100
                    elif metric == 'precision':
                        subgroup_asrs[val] = precision_score(y_att_query_subgroup, y_pred_subgroup) * 100
                    elif metric == 'recall':
                        subgroup_asrs[val] = recall_score(y_att_query_subgroup, y_pred_subgroup) * 100
                    elif metric == 'auc':
                        subgroup_asrs[val] = roc_auc_score(y_att_query_subgroup, y_pred_subgroup) * 100

            subgroup_disparity_dict[col] = subgroup_asrs


    return subgroup_disparity_dict







def plot_subgroup_disparity(subgroup_disparity_dict, title=None, req_metrics=None):
    # num_groups = len(subgroup_disparity_dict)

    for grp in subgroup_disparity_dict:
        metrics = list(subgroup_disparity_dict[grp].keys())
        if req_metrics is not None:
            metrics = [m for m in metrics if m in req_metrics]

        grp_vals = list(subgroup_disparity_dict[grp][metrics[0]].keys())
        # fig, axs = plt.subplots(1, 2, figsize=(10, 5))
        fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))


        if len(metrics) == 1:
            metric = metrics[0]
            axs.bar(grp_vals, subgroup_disparity_dict[grp][metric].values())
            axs.set_title(f'{metric} by {grp}')
        else:
            for i, metric in enumerate(metrics):
                axs[i].bar(grp_vals, subgroup_disparity_dict[grp][metric].values())
                axs[i].set_title(f'{metric} by {grp}')
        # Plot for 'sex'
        # sex_asrs = [data['sex']['asrs']['female'], data['sex']['asrs']['male']]
        # sex_asrs = subgroup_disparity_dict[grp]['asrs'].values()
        # sex_dists = [data['sex']['dists']['female'], data['sex']['dists']['male']]
        # sex_dists = subgroup_disparity_dict[grp]['dists'].values()

        # axs[0].bar(['Female', 'Male'], sex_asrs)
        # axs[0].bar(grp_vals, sex_asrs)
        # axs[0].set_title(f'ASRS by {grp}')

        # axs[1].bar(grp_vals, sex_dists)
        # axs[1].set_title(f'Dists by {grp}')


    plt.show()
    

# def get_top_dist_indices(ds, conditions, X_neuron, dist_percentile=0.9):
#     # normalize X_neuron first
#     X_neuron_mean = X_neuron.mean(axis=0)
#     X_neuron_std = X_neuron.std(axis=0)
#     X_neuron = (X_neuron - X_neuron_mean) / X_neuron_std
#     vec = subgroup_vulnerability_distance_vector(ds, conditions, X_neuron)
#     pos_vec = np.abs(vec)
#     sorted_indices = np.argsort(-pos_vec)
#     while True:
#         cum_norm = np.array([np.linalg.norm(vec[sorted_indices[:i]]) for i in range(1, len(sorted_indices)+1)])
#         cum_norm_p = cum_norm / cum_norm[-1]
#         idx = np.argmax(cum_norm_p > dist_percentile)
#         top_indices = sorted_indices[:idx]

#         if len(top_indices) > 0:
#             break
#         else:
#             dist_percentile += 0.1

#     return top_indices


def get_top_dist_indices(ds, conditions, X_neuron, dist_percentile=0.9):
    # normalize X_neuron first
    X_neuron_mean = X_neuron.mean(axis=0)
    X_neuron_std = X_neuron.std(axis=0)
    X_neuron = (X_neuron - X_neuron_mean) / X_neuron_std
    vec = subgroup_vulnerability_distance_vector(ds, conditions, X_neuron)
    # filter out the nan values
    nan_indices = np.argwhere(np.isnan(vec))
    vec = np.delete(vec, nan_indices)
    X_neuron = np.delete(X_neuron, nan_indices, axis=0)
    pos_vec = np.abs(vec)
    sorted_indices = np.argsort(-pos_vec)
    cum_norm = np.array([np.linalg.norm(vec[sorted_indices[:i]]) for i in range(1, len(sorted_indices)+1)])
    cum_norm_p = cum_norm / cum_norm[-1]
    idx = np.argmax(cum_norm_p > dist_percentile)
    top_indices = sorted_indices[:idx]

    return top_indices


def improved_subgroup_attack(ds, subgroup_columns, neuron_clf, X_neuron, y, dist_percentiles=[0.8, 0.9, 0.95, 0.99], metric='accuracy', minimum_indices=100, epochs=100):

    subgroup_disparity_dict = {}

    clfs = {}

    result_df = pd.DataFrame()

    x_n_tr, x_n_te, y_n_tr, y_n_te = train_test_split(X_neuron, y, test_size=0.5, random_state=42)

    row = {}
    for col in subgroup_columns:
        values = ds.ds.unique_values_dict[col]
        row['Subgroup Name'] = col
        
        subgroup_disparity_dict[col] = {}

        subgroup_disparity_dict[col]['original_asr'] = {}
        subgroup_disparity_dict[col]['improved_attack_asr'] = {}

        clfs[col] = {}

        for val in values:
            row['Subgroup Value'] = val
            if len(get_indices_by_conditions(ds, X_neuron, {col:val})) > minimum_indices:
                # subgroup_disparity_dict[col]['original_asr'][val] = get_accuracy(ds, {col:val}, neuron_clf, X_neuron, y)
                x_n_tr_norm, x_n_te_norm = normalize(x_n_tr, x_n_te)
                row['Original ASR'] = get_accuracy(ds, {col:val}, neuron_clf, x_n_te_norm, y_n_te, metric=metric)
                # subgroup_dists[val] = subgroup_vulnerability_distance(ds, {col:val}, X_neuron)
                
                for dist_percentile in dist_percentiles:
                    top_indices = get_top_dist_indices(ds, {col:val}, x_n_tr, dist_percentile=dist_percentile)


                    # clfs[col][val] = model_utils.get_model(max_iter=500)
                    y_onehot = ds.ds.sensitive_enc.transform(y_n_tr.to_numpy().ravel().reshape(-1,1)).toarray()
                    x_n_tr_sub, x_n_te_sub = x_n_tr.loc[:, top_indices], x_n_te.loc[:, top_indices]
                    x_n_tr_sub, x_n_te_sub = normalize(x_n_tr_sub, x_n_te_sub)
                    # clfs[col][val].fit(x_n_tr_sub, y_onehot)
                    clfs[col][val] = model_utils.proxy_train_mlp(x_n_tr_sub.to_numpy(), y_onehot, epochs=epochs)
                    # subgroup_disparity_dict[col]['improved_attack_asr'][val] = get_accuracy(ds, {col:val}, clfs[col][val], X_neuron[:, top_indices], y)
                    row[f'Improved Attack ASR ({dist_percentile})'] = get_accuracy(ds, {col:val}, clfs[col][val], x_n_te_sub, y_n_te, metric=metric)
                row['Marginal Prior'] = 100 * len(get_indices_by_conditions(ds, X_neuron, {col:val})) / len(X_neuron)
                # add the row to the result df
                result_df = pd.concat([result_df, pd.DataFrame([row])], ignore_index=True)
                


        # subgroup_disparity_dict[col] = {'asrs':subgroup_asrs, 'dists':subgroup_dists}

    return result_df, clfs
