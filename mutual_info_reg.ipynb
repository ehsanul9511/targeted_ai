{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import data_utils\n",
    "import model_utils\n",
    "from attack_utils import get_CSMIA_case_by_case_results, CSMIA_attack, LOMIA_attack\n",
    "from data_utils import oneHotCatVars, filter_random_data_by_conf_score\n",
    "from experiment_utils import MIAExperiment\n",
    "from disparity_inference_utils import get_confidence_array, draw_confidence_array_scatter, get_indices_by_group_condition, get_corr_btn_sens_and_out_per_subgroup, get_slopes, get_angular_difference, calculate_stds, get_mutual_info_btn_sens_and_out_per_subgroup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network._base import ACTIVATIONS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "import pickle\n",
    "# import utils\n",
    "import copy\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Setting the font family, size, and weight globally\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['font.weight'] = 'light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {(0, 1): 8750, (0, 0): 3750, (1, 1): 3750, (1, 0): 8750}, 1: {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {(0, 1): 8750, (0, 0): 3750, (1, 1): 3750, (1, 0): 8750}, 1: {(0, 1): 6875, (0, 0): 5625, (1, 1): 5625, (1, 0): 6875}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12500, 12500, 12500, 12500]\n"
     ]
    }
   ],
   "source": [
    "i = -0.4\n",
    "j = -0.1\n",
    "experiment = MIAExperiment(sampling_condition_dict = \n",
    "    {\n",
    "            'subgroup_col_name': 'SEX',\n",
    "            'n': 25000,\n",
    "            'correlation_by_subgroup_values': [i, j],\n",
    "            # 'fixed_corr_in_test_data': True\n",
    "    }, shortname = f\"Corr_btn_sens_and_output_for_male_({i})_for_female_({j})\", random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for experiment: Census19_subgroup_col_name_SEX_n_25000_correlation_by_subgroup_values_[-0.4, -0.1]_rs0\n",
      "Loaded classifier for experiment from file: Census19_subgroup_col_name_SEX_n_25000_correlation_by_subgroup_values_[-0.4, -0.1]_rs0\n"
     ]
    }
   ],
   "source": [
    "save_model=True\n",
    "print(f\"Training classifier for experiment: {experiment}\")\n",
    "try:\n",
    "    experiment.clf = model_utils.load_model(f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model.pkl')\n",
    "    print(f\"Loaded classifier for experiment from file: {experiment}\")\n",
    "except:\n",
    "    base_model = model_utils.get_model(max_iter=500)\n",
    "    experiment.clf = copy.deepcopy(base_model)\n",
    "    experiment.clf.fit(experiment.X_train, experiment.y_tr_onehot)\n",
    "\n",
    "    if save_model:\n",
    "        model_utils.save_model(experiment.clf, f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(torch.tensor(experiment.X_train.values).float(), torch.tensor(experiment.y_tr_onehot).float())\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(experiment.X_train.columns).index('SEX_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortedMLPClassifier(nn.Module):\n",
    "    def __init__(self, n_in_features=37, n_out_features=2):\n",
    "        super(PortedMLPClassifier, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_features=n_in_features, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=n_out_features),\n",
    "            nn.Softmax(dim=1)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def predict_proba(self, x: torch.Tensor):\n",
    "        return self.forward(x)\n",
    "    \n",
    "class MLPClassifierMutualInfoReg(nn.Module):\n",
    "    def __init__(self, n_in_features=37, n_feat_dim=10, n_out_features=2):\n",
    "        super(MLPClassifierMutualInfoReg, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_features=n_in_features, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=n_feat_dim),\n",
    "            # nn.Softmax(dim=1)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.k = n_feat_dim//2\n",
    "        self.st_layer = nn.Linear(in_features=n_feat_dim, out_features=self.k*2)\n",
    "        self.classifier = nn.Linear(in_features=self.k, out_features=n_out_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        statis = self.st_layer(x)\n",
    "        mu, std = statis[:, :self.k], statis[:, self.k:]\n",
    "        std = torch.functional.F.softplus(std-5)\n",
    "        eps = torch.FloatTensor(std.size()).normal_().to(x.device)\n",
    "        x = mu + eps * std\n",
    "        x = self.classifier(x)\n",
    "        x = self.softmax(x)\n",
    "        return x, mu, std\n",
    "    \n",
    "    def predict_proba(self, x: torch.Tensor):\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "def train_mir_classifier(model, train_loader, beta=0.1, selective_reg=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in tqdm(range(10)):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            data, target = data.to('mps'), target.to('mps')\n",
    "            output, mu, std = model(data)\n",
    "            info_loss = - 0.5 * (1 + 2 * (std+1e-7).log() - mu.pow(2) - std.pow(2)).sum(dim=1)\n",
    "            if selective_reg:\n",
    "                info_loss = info_loss * data[:, 38]\n",
    "            info_loss = info_loss.mean()\n",
    "            loss = nn.BCELoss()(output, target) + beta * info_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# test on test set\n",
    "def test_mir(model, X_test, y_te_onehot):\n",
    "    x_te = X_test.values\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(x_te).float(), torch.tensor(y_te_onehot).float())\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=x_te.shape[0], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to('mps'), target.to('mps')\n",
    "        output, _, _ = model(data)\n",
    "        y_pred.append(output.cpu().detach().numpy())\n",
    "        y_true.append(target.cpu().detach().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "betas = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "model_zero_by_beta = {}\n",
    "\n",
    "for beta in betas:\n",
    "    try:\n",
    "        model = MLPClassifierMutualInfoReg(n_in_features=experiment.X_train.shape[1], n_feat_dim=10, n_out_features=experiment.y_tr_onehot.shape[1]).to('mps')\n",
    "        model.load_state_dict(torch.load(f\"<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_mutual_info_reg_{beta}.pt\"))\n",
    "    except:\n",
    "        model = MLPClassifierMutualInfoReg(n_in_features=experiment.X_train.shape[1], n_feat_dim=10, n_out_features=experiment.y_tr_onehot.shape[1]).to('mps')\n",
    "        train_mir_classifier(model, train_loader, beta=beta)\n",
    "        torch.save(model.state_dict(), f\"<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_mutual_info_reg_{beta}.pt\")\n",
    "    \n",
    "    model_zero_by_beta[beta] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "betas = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "model_by_beta = {}\n",
    "\n",
    "for beta in betas:\n",
    "    try:\n",
    "        model = MLPClassifierMutualInfoReg(n_in_features=experiment.X_train.shape[1], n_feat_dim=10, n_out_features=experiment.y_tr_onehot.shape[1]).to('mps')\n",
    "        model.load_state_dict(torch.load(f\"<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_disp_aware_mutual_info_reg_{beta}.pt\"))\n",
    "    except:\n",
    "        model = MLPClassifierMutualInfoReg(n_in_features=experiment.X_train.shape[1], n_feat_dim=10, n_out_features=experiment.y_tr_onehot.shape[1]).to('mps')\n",
    "        train_mir_classifier(model, train_loader, beta=beta, selective_reg=True)\n",
    "        torch.save(model.state_dict(), f\"<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_disp_aware_mutual_info_reg_{beta}.pt\")\n",
    "    \n",
    "    model_by_beta[beta] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dict = {}\n",
    "\n",
    "subgroup_vals_tr = experiment.X_train[['SEX_0']].values.flatten()\n",
    "num_of_subgroups = 2\n",
    "\n",
    "for beta in model_zero_by_beta:\n",
    "    model = model_zero_by_beta[beta]\n",
    "    test_acc = test_mir(model, experiment.X_test, experiment.y_te_onehot)\n",
    "    sens_pred_CSMIA, _ = CSMIA_attack(model, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)\n",
    "    sens_pred_LOMIA = LOMIA_attack(experiment, model, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)\n",
    "    correct_indices_CSMIA = (sens_pred_CSMIA == experiment.sens_val_ground_truth)\n",
    "    correct_indices_LOMIA = (sens_pred_LOMIA == experiment.sens_val_ground_truth)\n",
    "    perf_dict[('zero', test_acc)] = {\n",
    "        'test_acc': test_acc,\n",
    "        'ASRD_CSMIA': round(100 * np.ptp([correct_indices_CSMIA[subgroup_vals_tr==i].mean() for i in range(num_of_subgroups)]), 2),\n",
    "        'ASRD_LOMIA': round(100 * np.ptp([correct_indices_LOMIA[subgroup_vals_tr==i].mean() for i in range(num_of_subgroups)]), 2),\n",
    "        'beta': beta,\n",
    "        'type': 'MIR'\n",
    "    }\n",
    "\n",
    "for beta in model_by_beta:\n",
    "    model = model_by_beta[beta]\n",
    "    test_acc = test_mir(model, experiment.X_test, experiment.y_te_onehot)\n",
    "    sens_pred_CSMIA, _ = CSMIA_attack(model, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)\n",
    "    sens_pred_LOMIA = LOMIA_attack(experiment, model, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)\n",
    "    correct_indices_CSMIA = (sens_pred_CSMIA == experiment.sens_val_ground_truth)\n",
    "    correct_indices_LOMIA = (sens_pred_LOMIA == experiment.sens_val_ground_truth)\n",
    "    perf_dict[('zero', test_acc)] = {\n",
    "        'test_acc': test_acc,\n",
    "        'ASRD_CSMIA': round(100 * np.ptp([correct_indices_CSMIA[subgroup_vals_tr==i].mean() for i in range(num_of_subgroups)]), 2),\n",
    "        'ASRD_LOMIA': round(100 * np.ptp([correct_indices_LOMIA[subgroup_vals_tr==i].mean() for i in range(num_of_subgroups)]), 2),\n",
    "        'beta': beta,\n",
    "        'type': 'DAMIR'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame.from_dict(perf_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>ASRD_CSMIA</th>\n",
       "      <th>ASRD_LOMIA</th>\n",
       "      <th>beta</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">zero</th>\n",
       "      <th>0.72594</th>\n",
       "      <td>0.72594</td>\n",
       "      <td>15.38</td>\n",
       "      <td>14.35</td>\n",
       "      <td>0.001</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.73786</th>\n",
       "      <td>0.73786</td>\n",
       "      <td>15.97</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.72486</th>\n",
       "      <td>0.72486</td>\n",
       "      <td>8.99</td>\n",
       "      <td>18.86</td>\n",
       "      <td>0.100</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.63254</th>\n",
       "      <td>0.63254</td>\n",
       "      <td>3.98</td>\n",
       "      <td>17.55</td>\n",
       "      <td>0.200</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45858</th>\n",
       "      <td>0.45858</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51934</th>\n",
       "      <td>0.51934</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.400</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.57708</th>\n",
       "      <td>0.57708</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.500</td>\n",
       "      <td>MIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.69614</th>\n",
       "      <td>0.69614</td>\n",
       "      <td>13.34</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.001</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.74882</th>\n",
       "      <td>0.74882</td>\n",
       "      <td>12.77</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.010</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.73604</th>\n",
       "      <td>0.73604</td>\n",
       "      <td>6.34</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.100</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71806</th>\n",
       "      <td>0.71806</td>\n",
       "      <td>6.12</td>\n",
       "      <td>21.49</td>\n",
       "      <td>0.200</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.67476</th>\n",
       "      <td>0.67476</td>\n",
       "      <td>1.19</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.300</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.67366</th>\n",
       "      <td>0.67366</td>\n",
       "      <td>1.60</td>\n",
       "      <td>6.69</td>\n",
       "      <td>0.400</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61480</th>\n",
       "      <td>0.61480</td>\n",
       "      <td>3.39</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.500</td>\n",
       "      <td>DAMIR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              test_acc  ASRD_CSMIA  ASRD_LOMIA   beta   type\n",
       "zero 0.72594   0.72594       15.38       14.35  0.001    MIR\n",
       "     0.73786   0.73786       15.97       15.16  0.010    MIR\n",
       "     0.72486   0.72486        8.99       18.86  0.100    MIR\n",
       "     0.63254   0.63254        3.98       17.55  0.200    MIR\n",
       "     0.45858   0.45858        0.34        0.44  0.300    MIR\n",
       "     0.51934   0.51934        0.24        0.47  0.400    MIR\n",
       "     0.57708   0.57708        0.39        2.26  0.500    MIR\n",
       "     0.69614   0.69614       13.34       14.76  0.001  DAMIR\n",
       "     0.74882   0.74882       12.77       15.25  0.010  DAMIR\n",
       "     0.73604   0.73604        6.34       14.31  0.100  DAMIR\n",
       "     0.71806   0.71806        6.12       21.49  0.200  DAMIR\n",
       "     0.67476   0.67476        1.19       13.32  0.300  DAMIR\n",
       "     0.67366   0.67366        1.60        6.69  0.400  DAMIR\n",
       "     0.61480   0.61480        3.39        5.74  0.500  DAMIR"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
